/**
 * Inflexion â€” Module d'embeddings locaux (transformers.js)
 *
 * GÃ©nÃ¨re des embeddings vectoriels Ã  partir de texte en utilisant
 * le modÃ¨le all-MiniLM-L6-v2 (384 dimensions) directement en Node.js.
 * ZÃ©ro API externe, zÃ©ro coÃ»t.
 *
 * Sprint 4 : IntÃ©gration du cache de persistance (embeddings-cache.mjs)
 * pour Ã©viter le recalcul des embeddings d'articles dÃ©jÃ  traitÃ©s.
 *
 * @requires @xenova/transformers
 */

import { pipeline, env } from '@xenova/transformers';
import {
    getCachedEmbedding,
    setCachedEmbedding,
    getCacheStats,
} from './embeddings-cache.mjs';

// Re-exporter les fonctions du cache pour usage par les scripts appelants
export {
    hashText,
    initEmbeddingsCache,
    saveEmbeddingsCache,
    getCacheStats,
    resetEmbeddingsCache,
    CACHE_TTL_MS,
} from './embeddings-cache.mjs';

// DÃ©sactiver le tÃ©lÃ©chargement de modÃ¨les Ã  distance si dÃ©jÃ  en cache
// Le modÃ¨le sera tÃ©lÃ©chargÃ© au premier usage puis mis en cache
env.cacheDir = './.cache/transformers';

// â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const MODEL_NAME = 'Xenova/all-MiniLM-L6-v2';
const EMBEDDING_DIM = 384;

let embedder = null;

// â”€â”€â”€ Initialisation (lazy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Initialise le pipeline d'embeddings (tÃ©lÃ©charge le modÃ¨le au premier appel).
 * Les appels suivants rÃ©utilisent l'instance en cache.
 * @returns {Promise<Function>}
 */
async function getEmbedder() {
    if (!embedder) {
        console.log(`  ğŸ”„ Chargement du modÃ¨le ${MODEL_NAME}...`);
        const start = Date.now();
        embedder = await pipeline('feature-extraction', MODEL_NAME, {
            quantized: true, // ModÃ¨le quantifiÃ© pour rÃ©duire la taille (~23MB vs ~90MB)
        });
        console.log(`  âœ“ ModÃ¨le chargÃ© en ${((Date.now() - start) / 1000).toFixed(1)}s`);
    }
    return embedder;
}

// â”€â”€â”€ API publique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * GÃ©nÃ¨re un embedding (vecteur 384D) pour un texte donnÃ©.
 * Utilise le cache si disponible (initEmbeddingsCache appelÃ© au prÃ©alable).
 *
 * @param {string} text â€” Le texte Ã  vectoriser
 * @returns {Promise<number[]>} â€” Vecteur de 384 dimensions (normalisÃ©)
 */
export async function embedText(text) {
    if (!text || typeof text !== 'string') {
        return new Array(EMBEDDING_DIM).fill(0);
    }

    // Tronquer Ã  ~256 tokens (~1024 caractÃ¨res) pour rester dans les limites du modÃ¨le
    const truncated = text.slice(0, 1024);

    // VÃ©rifier le cache
    const cached = getCachedEmbedding(truncated);
    if (cached) {
        return cached;
    }

    // Cache miss â€” calculer l'embedding
    const embed = await getEmbedder();

    const output = await embed(truncated, {
        pooling: 'mean',
        normalize: true,
    });

    const embedding = Array.from(output.data);

    // Stocker dans le cache
    setCachedEmbedding(truncated, embedding);

    return embedding;
}

/**
 * GÃ©nÃ¨re des embeddings pour un batch de textes.
 * Affiche les statistiques de cache et le temps Ã©coulÃ©.
 *
 * @param {string[]} texts â€” Les textes Ã  vectoriser
 * @param {Object} [options]
 * @param {Function} [options.onProgress] â€” Callback (index, total)
 * @returns {Promise<number[][]>} â€” Tableau de vecteurs 384D
 */
export async function embedBatch(texts, options = {}) {
    const { onProgress } = options;
    const start = Date.now();
    const results = [];

    for (let i = 0; i < texts.length; i++) {
        const vec = await embedText(texts[i]);
        results.push(vec);

        if (onProgress && i % 10 === 0) {
            onProgress(i, texts.length);
        }
    }

    const elapsed = ((Date.now() - start) / 1000).toFixed(1);
    const stats = getCacheStats();
    if (stats.entries > 0 || stats.hits > 0 || stats.misses > 0) {
        console.log(`  â± Batch embedding: ${texts.length} textes en ${elapsed}s (cache: ${stats.hits} hits, ${stats.misses} misses, ${stats.hitRate} hit rate)`);
    } else {
        console.log(`  â± Batch embedding: ${texts.length} textes en ${elapsed}s (pas de cache)`);
    }

    return results;
}

/**
 * Calcule la similaritÃ© cosinus entre deux vecteurs.
 * @param {number[]} a â€” Vecteur A
 * @param {number[]} b â€” Vecteur B
 * @returns {number} â€” Score entre -1 et 1 (1 = identique)
 */
export function cosineSimilarity(a, b) {
    if (!a || !b || a.length !== b.length) return 0;

    let dot = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
        dot += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }

    const denom = Math.sqrt(normA) * Math.sqrt(normB);
    return denom === 0 ? 0 : dot / denom;
}

/**
 * Dimension des embeddings gÃ©nÃ©rÃ©s.
 */
export const DIMENSION = EMBEDDING_DIM;
