/**
 * Inflexion â€” Module d'embeddings locaux (transformers.js)
 *
 * GÃ©nÃ¨re des embeddings vectoriels Ã  partir de texte en utilisant
 * le modÃ¨le all-MiniLM-L6-v2 (384 dimensions) directement en Node.js.
 * ZÃ©ro API externe, zÃ©ro coÃ»t.
 *
 * @requires @xenova/transformers
 */

import { pipeline, env } from '@xenova/transformers';

// DÃ©sactiver le tÃ©lÃ©chargement de modÃ¨les Ã  distance si dÃ©jÃ  en cache
// Le modÃ¨le sera tÃ©lÃ©chargÃ© au premier usage puis mis en cache
env.cacheDir = './.cache/transformers';

// â”€â”€â”€ Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

const MODEL_NAME = 'Xenova/all-MiniLM-L6-v2';
const EMBEDDING_DIM = 384;

let embedder = null;

// â”€â”€â”€ Initialisation (lazy) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * Initialise le pipeline d'embeddings (tÃ©lÃ©charge le modÃ¨le au premier appel).
 * Les appels suivants rÃ©utilisent l'instance en cache.
 * @returns {Promise<Function>}
 */
async function getEmbedder() {
    if (!embedder) {
        console.log(`  ðŸ”„ Chargement du modÃ¨le ${MODEL_NAME}...`);
        const start = Date.now();
        embedder = await pipeline('feature-extraction', MODEL_NAME, {
            quantized: true, // ModÃ¨le quantifiÃ© pour rÃ©duire la taille (~23MB vs ~90MB)
        });
        console.log(`  âœ“ ModÃ¨le chargÃ© en ${((Date.now() - start) / 1000).toFixed(1)}s`);
    }
    return embedder;
}

// â”€â”€â”€ API publique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

/**
 * GÃ©nÃ¨re un embedding (vecteur 384D) pour un texte donnÃ©.
 * @param {string} text â€” Le texte Ã  vectoriser
 * @returns {Promise<number[]>} â€” Vecteur de 384 dimensions (normalisÃ©)
 */
export async function embedText(text) {
    if (!text || typeof text !== 'string') {
        return new Array(EMBEDDING_DIM).fill(0);
    }

    const embed = await getEmbedder();

    // Tronquer Ã  ~256 tokens (~1024 caractÃ¨res) pour rester dans les limites du modÃ¨le
    const truncated = text.slice(0, 1024);

    const output = await embed(truncated, {
        pooling: 'mean',
        normalize: true,
    });

    // Convertir le tensor en tableau JS
    return Array.from(output.data);
}

/**
 * GÃ©nÃ¨re des embeddings pour un batch de textes.
 * @param {string[]} texts â€” Les textes Ã  vectoriser
 * @param {Object} [options]
 * @param {Function} [options.onProgress] â€” Callback (index, total)
 * @returns {Promise<number[][]>} â€” Tableau de vecteurs 384D
 */
export async function embedBatch(texts, options = {}) {
    const { onProgress } = options;
    const results = [];

    for (let i = 0; i < texts.length; i++) {
        const vec = await embedText(texts[i]);
        results.push(vec);

        if (onProgress && i % 10 === 0) {
            onProgress(i, texts.length);
        }
    }

    return results;
}

/**
 * Calcule la similaritÃ© cosinus entre deux vecteurs.
 * @param {number[]} a â€” Vecteur A
 * @param {number[]} b â€” Vecteur B
 * @returns {number} â€” Score entre -1 et 1 (1 = identique)
 */
export function cosineSimilarity(a, b) {
    if (!a || !b || a.length !== b.length) return 0;

    let dot = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
        dot += a[i] * b[i];
        normA += a[i] * a[i];
        normB += b[i] * b[i];
    }

    const denom = Math.sqrt(normA) * Math.sqrt(normB);
    return denom === 0 ? 0 : dot / denom;
}

/**
 * Dimension des embeddings gÃ©nÃ©rÃ©s.
 */
export const DIMENSION = EMBEDDING_DIM;
